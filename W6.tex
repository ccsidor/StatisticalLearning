\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% packages
\usepackage{amsmath, amsfonts}
\usepackage{graphicx, multicol}
\usepackage{textgreek,mathrsfs,bbm,relsize,multirow,float}
\usepackage{amssymb,amsthm,mathtools,scrextend,stackengine}
\usepackage[table,xcdraw]{xcolor} % clashes with tikz package
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{wrapfig}
\newenvironment{frcseries}{\fontfamily{frc}\selectfont}{}
%% commands
\newcommand{\xbar}{\mbox{\larger$\bar{x}$}}
\renewcommand{\baselinestretch}{1.5} %%1.5 spacing
\newcommand{\textfrc}[1]{{\frcseries#1}}
% commands

\newenvironment{sbmatrix}[1]
 {\def\mysubscript{#1}\mathop\bgroup\begin{bmatrix}}
 {\end{bmatrix}\egroup_{\textstyle\mathstrut\mysubscript}}
 
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\title{Stats Learning Lecture Week 6}

\begin{document}

\section{Week 6 - Notes}

data: crabs, ionosphere

\subsection{SVM \textbf{Jess}}
Ever look at data in Excel with more columns than the alphabet and wonder how you'll ever get through checking the significance of every variable? Fear no more, throw aside your boring regression model and don the exciting support vector machines (SVM) model! With the new and improved SVM, you can:
\begin{itemize}
\item Model and predict classes of data with a large number of attributes! *Kablam*
\item Model ranking data! *Peachy*
\item Model data when you have far more attributes than observations! *Wow*
\item Really like kernels but don't want to work as a movie theater concessionist your whole life! *Amazing*
\end{itemize}


SVM uses symmetrical margins around a decision boundary to determine which points are closer to the decision boundary. In general, the margin should be as large as possible so that the separation is as clean as possible. One disadvantage of using SVM is that there can be no missingness in the data, thus imputation is sometimes necessary. The decision boundary is determined as follows:

\begin{align*}
\text{SVM} \\
& h(x)=sign\Bigg(\sum_{i=1}^n\alpha_iy_ix^T_ix+b\Bigg)
\end{align*}

We want to maximize the margin such that:
\begin{align*}
\text{Margin}&=\text{distance}(H_{+1},H_{-1})\\
\text{Margin}&=\frac{2}{||w||}
\end{align*}

where w is the p-dimensional vector of the coefficients of the variables.

Thus we want to maximize the margin subject the the w that allows most all observations to be classified correctly. 

BUT WAIT THERE'S MORE! Call now, and you'll also get the amazing kernel extension to SVM! *Wowie!*

Using the kernel trick, SVM can make non-linear boundaries by projecting into a higher dimension. Kernel SVM also allows us to avoid directly computing the inner product $x^Tx$ by instead having us compute $\phi(x)^T\phi(x)$ as a kernel (similarity measure). In these cases, SVM is solved via quadratic programming. %\parencite{Seung-Seok2010AMeasures.}.


\begin{align*}
\text{Kernel SVM} \\
& h(x)=sign\Bigg(\sum_{i=1}^n\alpha_iy_i\phi(x_i)^T\phi(x)+b\Bigg)\\
&h(x)=sign\Bigg(\sum_{i=1}^n\alpha_iy_iK(x_i,x)+b\Bigg)
\end{align*}

There are numerous kernels that can be used for classification. Two kernels that will be used are the Gaussian Radial Basis Function (RBF or RBFdot) and the Laplace kernel.

\begin{align*}
\text{RBF} \\
& K(x_i,x_j)=exp(-\delta||x_i-x_j||_2^2)\\
&\delta\equiv\text{bandwidth}\\
\text{Laplace} \\
& K(x_i,x_j)=exp(-\delta||x_i-x_j||_1)\\
&\delta\equiv \text{(usually) inverse of the number of predictors}
\end{align*}

Another note of interest in SVM is that along with changing the kernel, you can also change how you calculate the bandwidth, $\delta$.  

\subsection{LDA/QDA \textbf{Jess}}

Let's say you've plotted out your data on a few different dimensions and noticed that there's clear clusters. Trees or SVM might be able to handle this, but another tool to add to your arsenal is linear discriminant analysis, LDA.

Similar to SVM, LDA attempts to classify data by separating it with a linear line. LDA does this because it assumes that the data is created from two different normal distributions with equal covariances. LDA models the both "sides" of the data with (nearly) the same covariances and different means and figures out the optimal place to put a line between the two distributions to minimize error.

LDA requires that each class has a different mean, $\mu$, but the same covariance, $\Sigma$, and defines each class as having a probability of occurrence as $\pi_{j}=\text{Pr}(Y=j)$.

The rejection of normality will not normally hurt LDA's chance of prediction and modeling since LDA is robust. However with the rejection of normality, other methods become available for use such as QDA, quadratic discriminant analysis, or kernel discrimination which could now potentially model the data better. However, if the covariances are significantly different, we really should not be using LDA. LDA only works under the assumption of equal variances for the groupings.


Mathematically, LDA says to assign an observation to class $j$ if:

$$\delta_{j}(x)=x^{T}\hat{b}^{x}_{j}+c^{x}_{j}$$

Let's say you can still clearly see clusters in the data but know that there is no single line that you could place in the graphs to separate the clusters. Now we can use qda, quadratic discriminant analysis, to draw parabolas and more around our data to find the proper curvy boundary. 

\subsection{RDA \textbf{Chris}}
\begin{itemize}
\item why
\item math
\item graph
\item algorithm
\end{itemize}

* 1 to 2 Data sets in R/Python as examples

\subsection{Week 6 - Homeworks}
compare and run through lda/qda/rda/svm
pick best kernel of bag of kernels
more matching to math def
compare outcomes via different metrics

\subsection{Week 6 - AIOS}


* paper on QP instead of lecturing on it

\end{document}

