\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% packages
\usepackage{amsmath, amsfonts}
\usepackage{graphicx, multicol}
\usepackage{textgreek,mathrsfs,bbm,relsize,multirow,float}
\usepackage{amssymb,amsthm,mathtools,scrextend,stackengine}
\usepackage[table,xcdraw]{xcolor} % clashes with tikz package
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{wrapfig}
\newenvironment{frcseries}{\fontfamily{frc}\selectfont}{}
%% commands
\newcommand{\xbar}{\mbox{\larger$\bar{x}$}}
\renewcommand{\baselinestretch}{1.5} %%1.5 spacing
\newcommand{\textfrc}[1]{{\frcseries#1}}
% commands

\newenvironment{sbmatrix}[1]
 {\def\mysubscript{#1}\mathop\bgroup\begin{bmatrix}}
 {\end{bmatrix}\egroup_{\textstyle\mathstrut\mysubscript}}
 
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\title{Stats Learning Lecture Week 6}

\begin{document}

\section{Week 6 - Notes}

data: crabs, ionosphere

\subsection{SVM}
Ever look at data in Excel with more columns than the alphabet and wonder how you'll ever get through checking the significance of every variable? Fear no more, throw aside your boring regression model and don the exciting support vector machines (SVM) model! With the new and improved SVM, you can:
\begin{itemize}
\item Model and predict classes of data with a large number of attributes! *Kablam*
\item Model ranking data! *Peachy*
\item Model data when you have far more attributes than observations! *Wow*
\item Really like kernels but don't want to work as a movie theater concessionist your whole life! *Amazing*
\end{itemize}


SVM uses symmetrical margins around a decision boundary to determine which points are closer to the decision boundary. In general, the margin should be as large as possible so that the separation is as clean as possible. One disadvantage of using SVM is that there can be no missingness in the data, thus imputation is sometimes necessary. The decision boundary is determined as follows:

\begin{align*}
\text{SVM} \\
& h(x)=sign\Bigg(\sum_{i=1}^n\alpha_iy_ix^T_ix+b\Bigg)
\end{align*}

We want to maximize the margin such that:
\begin{align*}
\text{Margin}&=\text{distance}(H_{+1},H_{-1})\\
\text{Margin}&=\frac{2}{||w||}
\end{align*}

where w is the p-dimensional vector of the coefficients of the variables.

Thus we want to maximize the margin subject the the w that allows most all observations to be classified correctly. 

BUT WAIT THERE'S MORE! Call now, and you'll also get the amazing kernel extension to SVM! *Wowie!*

Using the kernel trick, SVM can make non-linear boundaries by projecting into a higher dimension. Kernel SVM also allows us to avoid directly computing the inner product $x^Tx$ by instead having us compute $\phi(x)^T\phi(x)$ as a kernel (similarity measure). In these cases, SVM is solved via quadratic programming. %\parencite{Seung-Seok2010AMeasures.}.


\begin{align*}
\text{Kernel SVM} \\
& h(x)=sign\Bigg(\sum_{i=1}^n\alpha_iy_i\phi(x_i)^T\phi(x)+b\Bigg)\\
&h(x)=sign\Bigg(\sum_{i=1}^n\alpha_iy_iK(x_i,x)+b\Bigg)
\end{align*}

There are numerous kernels that can be used for classification. Two kernels that will be used are the Gaussian Radial Basis Function (RBF or RBFdot) and the Laplace kernel.

\begin{align*}
\text{RBF} \\
& K(x_i,x_j)=exp(-\delta||x_i-x_j||_2^2)\\
&\delta\equiv\text{bandwidth}\\
\text{Laplace} \\
& K(x_i,x_j)=exp(-\delta||x_i-x_j||_1)\\
&\delta\equiv \text{(usually) inverse of the number of predictors}
\end{align*}

Another note of interest in SVM is that along with changing the kernel, you can also change how you calculate the bandwidth, $\delta$.  

\subsection{LDA/QDA}

Let's say you've plotted out your data on a few different dimensions and noticed that there's clear clusters. Trees or SVM might be able to handle this, but another tool to add to your arsenal is linear discriminant analysis, LDA.

Similar to SVM, LDA attempts to classify data by separating it with a linear line. LDA does this because it assumes that the data is created from two different normal distributions with equal covariances. LDA models the both "sides" of the data with (nearly) the same covariances and different means and figures out the optimal place to put a line between the two distributions to minimize error.

LDA requires that each class has a different mean, $\mu$, but the same covariance, $\Sigma$, and defines each class as having a probability of occurrence as $\pi_{j}=\text{Pr}(Y=j)$.

The rejection of normality will not normally hurt LDA's chance of prediction and modeling since LDA is robust. However with the rejection of normality, other methods become available for use such as QDA, quadratic discriminant analysis, or kernel discrimination which could now potentially model the data better. However, if the covariances are significantly different, we really should not be using LDA. LDA only works under the assumption of equal variances for the groupings.


Mathematically, LDA says to assign an observation to class $j$ if:

$$\delta_{j}(x)=x^{T}\hat{b}^{x}_{j}+c^{x}_{j}$$

Let's say you can still clearly see clusters in the data but know that there is no single line that you could place in the graphs to separate the clusters. Now we can use qda, quadratic discriminant analysis, to draw parabolas and more around our data to find the proper curvy boundary. 

\section{Regularized discriminant analysis (RDA)}
Regularized discriminant analysis (RDA) is a compromise between LDA and QDA, which allows one to shrink the separate covariances of QDA toward a common variance as in LDA. This method is very similar in flavor to ridge regression. The regularized covariance matrices of each \begin{equation}
\Sigma_k(\alpha)=\alpha\Sigma_k+(1-\alpha)\Sigma. 
\end{equation}

The quadratic discriminant function is defined using the shrunken covariance matrices $\Sigma_k(\alpha)$. The parameter $\alpha \in [0, 1]$ controls the complexity of the model. When $\alpha$ is one, RDA becomes QDA. While $\alpha$ is zero, RDA is equivalent to LDA. Therefore, the regularization factor $\alpha$ allows a continuum of models between LDA and QDA.

\subsection{Class example}
The following plot shows you how the error rate within training data and test data changes when the parameter $\alpha$ changes. It also shows you the trade off between model complexity and the fitting of the trading data.

\begin{figure}[H]
\center
\includegraphics[width=15cm]{RDA_change_alpha_plot.png}
\caption{Errors for RDA method with varying $\alpha$.$^{1}$}
\label{dharmaweb1}
\end{figure}

Basically, values of $\alpha$ from 0 to 1 are applied using RDA. In every case the error rate within the training data is computed (lower green dotted line) and the error rate within a separate test data is also computed (upper red dotted line). You can see that when $\alpha$ becomes closer to 1, which means the model becomes more and more flexible - more like QDA, the error rate decreases in training data. If your model is more flexible you can fit the training data better, therefore, the error rate within the training set decreases. It is a different story with the test data. You can see with the test data, when you increase $\alpha$,  the error at first decreases. Then, if you make $\alpha$ too close to 1, the error rate increases. This shows you the trade-off between model complexity and the fitting of the training data.


\subsection{Papers from Chris}
Regularized Discriminant Analysis Author(s): Jerome H. Friedman
Source: Journal of the American Statistical Association, Vol. 84, No. 405 (Mar., 1989), pp. 165- 175


\subsection{Citations}
[1] Graph from ``The Elements of Statistical Learning'', 2nd edition, by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. NOTE: We will create  class code so that we can generate it ourselves, so placeholder only.


\subsection{Week 6 - Homeworks}
compare and run through lda/qda/rda/svm
more matching to math def
compare outcomes via different metrics

\subsection{Week 6 - AIOS}
Dynamic generation of the above graph? Slider to set $\alpha$?

* paper on QP instead of lecturing on it

\end{document}
